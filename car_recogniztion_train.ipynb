{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ultralytics -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm.notebook import tqdm\n",
    "import IPython\n",
    "from IPython.display import Video, display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'https://docs.google.com/uc?export=download&confirm=&id=1pz68D1Gsx80MoPg-_q-IbEdESEmyVLm-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://docs.google.com/uc?export=download&confirm=&id=1pz68D1Gsx80MoPg-_q-IbEdESEmyVLm-\" controls  width=\"832\"  height=\"468\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frac = 0.65 \n",
    "display(Video(data=path, height=int(720*frac), width=int(1280*frac)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8x.pt')\n",
    "dict_classes = model.model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risize_frame(frame, scale_percent):\n",
    "    \"\"\"Function to resize an image in a percent scale\"\"\"\n",
    "    width = int(frame.shape[1] * scale_percent / 100)\n",
    "    height = int(frame.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "\n",
    "    # resize image\n",
    "    resized = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Verbose during Prediction: False\n",
      "[INFO] - Original Dim:  (3840, 2160)\n",
      "[INFO] - Scaling change may cause errors in pixels lines \n",
      "[INFO] - Dim Scaled:  (1920, 1080)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61deeb53abd6476f841dfca4515fb3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/538 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.200  Python-3.11.6 torch-2.1.0+cpu \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n\ntorch.cuda.is_available(): False\ntorch.cuda.device_count(): 0\nos.environ['CUDA_VISIBLE_DEVICES']: None\nSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Python\\car_recogniztion_train.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Python/car_recogniztion_train.ipynb#W6sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDimension Scaled(frame): \u001b[39m\u001b[39m'\u001b[39m, (frame\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], frame\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Python/car_recogniztion_train.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Getting predictions\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Python/car_recogniztion_train.ipynb#W6sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m y_hat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(frame, conf \u001b[39m=\u001b[39;49m \u001b[39m0.7\u001b[39;49m, classes \u001b[39m=\u001b[39;49m class_IDS, device \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, verbose \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Python/car_recogniztion_train.ipynb#W6sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# Getting the bounding boxes, confidence and classes of the recognize objects in the current frame.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Python/car_recogniztion_train.ipynb#W6sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m boxes   \u001b[39m=\u001b[39m y_hat[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mboxes\u001b[39m.\u001b[39mxyxy\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\model.py:235\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor:\n\u001b[0;32m    234\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor \u001b[39m=\u001b[39m (predictor \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_smart_load(\u001b[39m'\u001b[39m\u001b[39mpredictor\u001b[39m\u001b[39m'\u001b[39m))(overrides\u001b[39m=\u001b[39margs, _callbacks\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks)\n\u001b[1;32m--> 235\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor\u001b[39m.\u001b[39;49msetup_model(model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, verbose\u001b[39m=\u001b[39;49mis_cli)\n\u001b[0;32m    236\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# only update args if predictor is already setup\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m get_cfg(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs, args)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:312\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[1;34m(self, model, verbose)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup_model\u001b[39m(\u001b[39mself\u001b[39m, model, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    310\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AutoBackend(model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m--> 312\u001b[0m                              device\u001b[39m=\u001b[39mselect_device(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdevice, verbose\u001b[39m=\u001b[39;49mverbose),\n\u001b[0;32m    313\u001b[0m                              dnn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdnn,\n\u001b[0;32m    314\u001b[0m                              data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdata,\n\u001b[0;32m    315\u001b[0m                              fp16\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhalf,\n\u001b[0;32m    316\u001b[0m                              fuse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    317\u001b[0m                              verbose\u001b[39m=\u001b[39mverbose)\n\u001b[0;32m    319\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mdevice  \u001b[39m# update device\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhalf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfp16  \u001b[39m# update half\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\utils\\torch_utils.py:119\u001b[0m, in \u001b[0;36mselect_device\u001b[1;34m(device, batch, newline, verbose)\u001b[0m\n\u001b[0;32m    116\u001b[0m         LOGGER\u001b[39m.\u001b[39minfo(s)\n\u001b[0;32m    117\u001b[0m         install \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no \u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[0;32m    118\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mCUDA devices are seen by torch.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid CUDA \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdevice=\u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m requested.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Use \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdevice=cpu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or pass valid CUDA device(s) if available,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m i.e. \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdevice=0\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdevice=0,1,2,3\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for Multi-GPU.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mtorch.cuda.is_available(): \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    123\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mtorch.cuda.device_count(): \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    124\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mos.environ[\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[39m'\u001b[39m\u001b[39m]: \u001b[39m\u001b[39m{\u001b[39;00mvisible\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m                          \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00minstall\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m cpu \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m mps \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():  \u001b[39m# prefer GPU if available\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     devices \u001b[39m=\u001b[39m device\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m device \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# range(torch.cuda.device_count())  # i.e. 0,1,6,7\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n\ntorch.cuda.is_available(): False\ntorch.cuda.device_count(): 0\nos.environ['CUDA_VISIBLE_DEVICES']: None\nSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n"
     ]
    }
   ],
   "source": [
    "### Configurations\n",
    "#Verbose during prediction\n",
    "verbose = False\n",
    "# Scaling percentage of original frame\n",
    "scale_percent = 50\n",
    "\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# Reading video with cv2\n",
    "video = cv2.VideoCapture(path)\n",
    "\n",
    "# Objects to detect Yolo\n",
    "class_IDS = [2, 3, 5, 7] \n",
    "# Auxiliary variables\n",
    "centers_old = {}\n",
    "centers_new = {}\n",
    "obj_id = 0 \n",
    "veiculos_contador_in = dict.fromkeys(class_IDS, 0)\n",
    "veiculos_contador_out = dict.fromkeys(class_IDS, 0)\n",
    "end = []\n",
    "frames_list = []\n",
    "cy_linha = int(1500 * scale_percent/100 )\n",
    "cx_sentido = int(2000 * scale_percent/100) \n",
    "offset = int(8 * scale_percent/100 )\n",
    "contador_in = 0\n",
    "contador_out = 0\n",
    "print(f'[INFO] - Verbose during Prediction: {verbose}')\n",
    "\n",
    "\n",
    "# Original informations of video\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "print('[INFO] - Original Dim: ', (width, height))\n",
    "\n",
    "# Scaling Video for better performance \n",
    "if scale_percent != 100:\n",
    "    print('[INFO] - Scaling change may cause errors in pixels lines ')\n",
    "    width = int(width * scale_percent / 100)\n",
    "    height = int(height * scale_percent / 100)\n",
    "    print('[INFO] - Dim Scaled: ', (width, height))\n",
    "    \n",
    "\n",
    "#-------------------------------------------------------\n",
    "### Video output ####\n",
    "video_name = 'result.mp4'\n",
    "output_path = \"rep_\" + video_name\n",
    "tmp_output_path = \"tmp_\" + output_path\n",
    "VIDEO_CODEC = \"MP4V\"\n",
    "\n",
    "output_video = cv2.VideoWriter(tmp_output_path, \n",
    "                               cv2.VideoWriter_fourcc(*VIDEO_CODEC), \n",
    "                               fps, (width, height))\n",
    "\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# Executing Recognition \n",
    "for i in tqdm(range(int(video.get(cv2.CAP_PROP_FRAME_COUNT)))):\n",
    "    \n",
    "    # reading frame from video\n",
    "    _, frame = video.read()\n",
    "    \n",
    "    #Applying resizing of read frame\n",
    "    frame  = risize_frame(frame, scale_percent)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Dimension Scaled(frame): ', (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # Getting predictions\n",
    "    y_hat = model.predict(frame, conf = 0.7, classes = class_IDS, device = 0, verbose = False)\n",
    "    \n",
    "    # Getting the bounding boxes, confidence and classes of the recognize objects in the current frame.\n",
    "    boxes   = y_hat[0].boxes.xyxy.cpu().numpy()\n",
    "    conf    = y_hat[0].boxes.conf.cpu().numpy()\n",
    "    classes = y_hat[0].boxes.cls.cpu().numpy() \n",
    "    \n",
    "    # Storing the above information in a dataframe\n",
    "    positions_frame = pd.DataFrame(y_hat[0].cpu().numpy().boxes.boxes, columns = ['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])\n",
    "    \n",
    "    #Translating the numeric class labels to text\n",
    "    labels = [dict_classes[i] for i in classes]\n",
    "    \n",
    "    # Drawing transition line for in\\out vehicles counting \n",
    "    cv2.line(frame, (0, cy_linha), (int(4500 * scale_percent/100 ), cy_linha), (255,255,0),8)\n",
    "    \n",
    "    # For each vehicles, draw the bounding-box and counting each one the pass thought the transition line (in\\out)\n",
    "    for ix, row in enumerate(positions_frame.iterrows()):\n",
    "        # Getting the coordinates of each vehicle (row)\n",
    "        xmin, ymin, xmax, ymax, confidence, category,  = row[1].astype('int')\n",
    "        \n",
    "        # Calculating the center of the bounding-box\n",
    "        center_x, center_y = int(((xmax+xmin))/2), int((ymax+ ymin)/2)\n",
    "        \n",
    "        # drawing center and bounding-box of vehicle in the given frame \n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255,0,0), 5) # box\n",
    "        cv2.circle(frame, (center_x,center_y), 5,(255,0,0),-1) # center of box\n",
    "        \n",
    "        #Drawing above the bounding-box the name of class recognized.\n",
    "        cv2.putText(img=frame, text=labels[ix]+' - '+str(np.round(conf[ix],2)),\n",
    "                    org= (xmin,ymin-10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 0, 0),thickness=2)\n",
    "        \n",
    "        # Checking if the center of recognized vehicle is in the area given by the transition line + offset and transition line - offset \n",
    "        if (center_y < (cy_linha + offset)) and (center_y > (cy_linha - offset)):\n",
    "            if  (center_x >= 0) and (center_x <=cx_sentido):\n",
    "                contador_in +=1\n",
    "                veiculos_contador_in[category] += 1\n",
    "            else:\n",
    "                contador_out += 1\n",
    "                veiculos_contador_out[category] += 1\n",
    "    \n",
    "    #updating the counting type of vehicle \n",
    "    contador_in_plt = [f'{dict_classes[k]}: {i}' for k, i in veiculos_contador_in.items()]\n",
    "    contador_out_plt = [f'{dict_classes[k]}: {i}' for k, i in veiculos_contador_out.items()]\n",
    "    \n",
    "    #drawing the number of vehicles in\\out\n",
    "    cv2.putText(img=frame, text='N. vehicles In', \n",
    "                org= (30,30), fontFace=cv2.FONT_HERSHEY_TRIPLEX, \n",
    "                fontScale=1, color=(255, 255, 0),thickness=1)\n",
    "    \n",
    "    cv2.putText(img=frame, text='N. vehicles Out', \n",
    "                org= (int(2800 * scale_percent/100 ),30), \n",
    "                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=1)\n",
    "    \n",
    "    #drawing the counting of type of vehicles in the corners of frame \n",
    "    xt = 40\n",
    "    for txt in range(len(contador_in_plt)):\n",
    "        xt +=30\n",
    "        cv2.putText(img=frame, text=contador_in_plt[txt], \n",
    "                    org= (30,xt), fontFace=cv2.FONT_HERSHEY_TRIPLEX, \n",
    "                    fontScale=1, color=(255, 255, 0),thickness=1)\n",
    "        \n",
    "        cv2.putText(img=frame, text=contador_out_plt[txt],\n",
    "                    org= (int(2800 * scale_percent/100 ),xt), fontFace=cv2.FONT_HERSHEY_TRIPLEX,\n",
    "                    fontScale=1, color=(255, 255, 0),thickness=1)\n",
    "    \n",
    "    #drawing the number of vehicles in\\out\n",
    "    cv2.putText(img=frame, text=f'In:{contador_in}', \n",
    "                org= (int(1820 * scale_percent/100 ),cy_linha+60),\n",
    "                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=2)\n",
    "    \n",
    "    cv2.putText(img=frame, text=f'Out:{contador_out}', \n",
    "                org= (int(1800 * scale_percent/100 ),cy_linha-40),\n",
    "                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=2)\n",
    "\n",
    "    if verbose:\n",
    "        print(contador_in, contador_out)\n",
    "    #Saving frames in a list \n",
    "    frames_list.append(frame)\n",
    "    #saving transformed frames in a output video formaat\n",
    "    output_video.write(frame)\n",
    "    \n",
    "#Releasing the video    \n",
    "output_video.release()\n",
    "\n",
    "\n",
    "####  pos processing\n",
    "# Fixing video output codec to run in the notebook\\browser\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "    \n",
    "subprocess.run(\n",
    "    [\"ffmpeg\",  \"-i\", tmp_output_path,\"-crf\",\"18\",\"-preset\",\"veryfast\",\"-hide_banner\",\"-loglevel\",\"error\",\"-vcodec\",\"libx264\",output_path])\n",
    "os.remove(tmp_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
